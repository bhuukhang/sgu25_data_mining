"""
K-NEAREST NEIGHBORS - PHÃ‚N LOáº I CHá»® CÃI (LETTER RECOGNITION)
Dataset: UCI Letter Recognition
- 26 classes: A to Z
- 16 features: statistical moments and edge counts
- 20,000 samples
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score
import warnings
warnings.filterwarnings('ignore')

# ==================== BÆ¯á»šC 1: THU THáº¬P VÃ€ CHUáº¨N Bá»Š Dá»® LIá»†U ====================
print("=" * 80)
print("BÆ¯á»šC 1: THU THáº¬P VÃ€ CHUáº¨N Bá»Š Dá»® LIá»†U")
print("=" * 80)

# Äá»c dá»¯ liá»‡u tá»« file locall
print("\nğŸ“¥ Äang Ä‘á»c dá»¯ liá»‡u tá»« file local...")

# Äáº·t tÃªn file cá»§a báº¡n táº¡i Ä‘Ã¢y
FILE_PATH = "/Users/tuong/Documents/MONHOCDAIHOC/Khai phÃ¡ data/ai_practice_prj/fai_practice_prj/p01_KNN/codep01/letter/letter-recognition.data"  # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n náº¿u cáº§n

# TÃªn cÃ¡c cá»™t theo documentation cá»§a UCI
column_names = ['letter', 'x-box', 'y-box', 'width', 'height', 'onpix', 
                'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 
                'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx']

try:
    # Äá»c file CSV/TXT
    df = pd.read_csv(FILE_PATH, names=column_names, header=None)
    X = df.iloc[:, 1:].values  # Táº¥t cáº£ features (cá»™t 2-17)
    y = df.iloc[:, 0].values    # Labels (cá»™t 1: chá»¯ cÃ¡i)
    print(f"âœ“ Äá»c dá»¯ liá»‡u thÃ nh cÃ´ng tá»« file: {FILE_PATH}")
except FileNotFoundError:
    print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {FILE_PATH}")
    print("ğŸ“Œ Vui lÃ²ng:")
    print("   1. Äáº£m báº£o file 'etter-recognition.data' náº±m cÃ¹ng thÆ° má»¥c vá»›i code")
    print("   2. Hoáº·c thay Ä‘á»•i FILE_PATH thÃ nh Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n file")
    print("   VÃ­ dá»¥: FILE_PATH = 'C:/Users/YourName/Desktop/letter-recognition.data'")
    exit()
except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c file: {e}")
    exit()

# Táº¡o DataFrame Ä‘á»ƒ xem
feature_names = ['x-box', 'y-box', 'width', 'height', 'onpix', 
                 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 
                 'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx']

df = pd.DataFrame(X, columns=feature_names)
df['letter'] = y

print(f"\nğŸ“Š ThÃ´ng tin dá»¯ liá»‡u:")
print(f"- Sá»‘ máº«u: {len(df)}")
print(f"- Sá»‘ Ä‘áº·c trÆ°ng: {X.shape[1]}")
print(f"- Sá»‘ classes (A-Z): {len(np.unique(y))}")
print(f"- Classes: {sorted(np.unique(y))}")

print(f"\nğŸ“‹ 5 máº«u Ä‘áº§u tiÃªn:")
print(df.head())

print(f"\nğŸ“ˆ Thá»‘ng kÃª mÃ´ táº£:")
print(df.describe())

print(f"\nğŸ” Kiá»ƒm tra dá»¯ liá»‡u thiáº¿u:")
print(f"Sá»‘ giÃ¡ trá»‹ null: {df.isnull().sum().sum()}")

print(f"\nğŸ“Š PhÃ¢n bá»‘ cÃ¡c chá»¯ cÃ¡i (5 chá»¯ Ä‘áº§u):")
print(df['letter'].value_counts().head())


# ==================== BÆ¯á»šC 2: TIá»€N Xá»¬ LÃ Dá»® LIá»†U ====================
print("\n" + "=" * 80)
print("BÆ¯á»šC 2: TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
print("=" * 80)

# 2.1: Kiá»ƒm tra vÃ  xá»­ lÃ½ dÃ²ng trÃ¹ng TRÆ¯á»šC KHI chia dá»¯ liá»‡u
print("\n[2.1] Kiá»ƒm tra dÃ²ng trÃ¹ng:")
duplicates_count = df.duplicated().sum()
print(f"Sá»‘ dÃ²ng trÃ¹ng: {duplicates_count}")

if duplicates_count > 0:
    print(f"âœ“ TÃ¬m tháº¥y {duplicates_count} dÃ²ng trÃ¹ng")
    df = df.drop_duplicates()
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values
    print(f"âœ“ Sau khi xÃ³a trÃ¹ng: {len(df)} máº«u")
else:
    print("âœ“ KhÃ´ng cÃ³ dÃ²ng trÃ¹ng trong dataset")

# 2.2: Encode labels (chuyá»ƒn chá»¯ cÃ¡i thÃ nh sá»‘)
print("\n[2.2] Encode labels (A-Z â†’ 0-25):")
print(f"Labels trÆ°á»›c khi encode: {sorted(np.unique(y))[:5]}... (hiá»ƒn thá»‹ 5 chá»¯ Ä‘áº§u)")

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"Labels sau khi encode: {np.unique(y_encoded)[:5]}... (0-25)")
print(f"âœ“ Mapping vÃ­ dá»¥: A={label_encoder.transform(['A'])[0]}, B={label_encoder.transform(['B'])[0]}, Z={label_encoder.transform(['Z'])[0]}")

# Cáº­p nháº­t y
y = y_encoded

print(f"âœ“ Shape cá»§a X: {X.shape}")
print(f"âœ“ Shape cá»§a y: {y.shape}")

# 2.3: Chia dá»¯ liá»‡u train/test (70/30)
print("\n[2.3] Chia dá»¯ liá»‡u train/test (70/30):")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"âœ“ Táº­p train: {len(X_train)} máº«u ({len(X_train)/len(X)*100:.1f}%)")
print(f"âœ“ Táº­p test: {len(X_test)} máº«u ({len(X_test)/len(X)*100:.1f}%)")

# 2.4: Chuáº©n hÃ³a CHá»ˆ features (X)
print("\n[2.4] Chuáº©n hÃ³a features (StandardScaler):")
print("âš ï¸  CHá»ˆ chuáº©n hÃ³a features (X), KHÃ”NG chuáº©n hÃ³a labels (y)!")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"âœ“ Mean cá»§a X_train sau chuáº©n hÃ³a: {X_train_scaled.mean(axis=0)[:3].round(2)}... (gáº§n 0)")
print(f"âœ“ Std cá»§a X_train sau chuáº©n hÃ³a: {X_train_scaled.std(axis=0)[:3].round(2)}... (gáº§n 1)")


# ==================== BÆ¯á»šC 3: CÃ€I Äáº¶T THUáº¬T TOÃN KNN ====================
print("\n" + "=" * 80)
print("BÆ¯á»šC 3: CÃ€I Äáº¶T THUáº¬T TOÃN K-NEAREST NEIGHBORS")
print("=" * 80)

class KNearestNeighbors:
    """CÃ i Ä‘áº·t thuáº­t toÃ¡n K-Nearest Neighbors tá»« Ä‘áº§u"""
    
    def __init__(self, k=3):
        self.k = k
        self.X_train = None
        self.y_train = None
    
    def fit(self, X_train, y_train):
        """LÆ°u dá»¯ liá»‡u training"""
        self.X_train = X_train
        self.y_train = y_train
        print(f"âœ“ ÄÃ£ lÆ°u {len(X_train)} máº«u training")
    
    def euclidean_distance(self, x1, x2):
        """TÃ­nh khoáº£ng cÃ¡ch Euclidean"""
        return np.sqrt(np.sum((x1 - x2) ** 2))
    
    def predict_single(self, x):
        """Dá»± Ä‘oÃ¡n cho 1 Ä‘iá»ƒm dá»¯ liá»‡u"""
        # TÃ­nh khoáº£ng cÃ¡ch Ä‘áº¿n táº¥t cáº£ Ä‘iá»ƒm training
        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]
        
        # Láº¥y k Ä‘iá»ƒm gáº§n nháº¥t
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = self.y_train[k_indices]
        
        # Voting: chá»n label xuáº¥t hiá»‡n nhiá»u nháº¥t
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]
    
    def predict(self, X):
        """Dá»± Ä‘oÃ¡n cho nhiá»u Ä‘iá»ƒm dá»¯ liá»‡u"""
        predictions = []
        total = len(X)
        for i, x in enumerate(X):
            if (i + 1) % 500 == 0 or i == 0:
                print(f"  Äang dá»± Ä‘oÃ¡n: {i+1}/{total} máº«u ({(i+1)/total*100:.1f}%)")
            predictions.append(self.predict_single(x))
        return np.array(predictions)

# Khá»Ÿi táº¡o vá»›i k=5 (tá»‘t hÆ¡n cho 26 classes)
knn = KNearestNeighbors(k=5)
print(f"\nâœ“ Khá»Ÿi táº¡o mÃ´ hÃ¬nh KNN vá»›i k={knn.k}")
print(f"  (Vá»›i 26 classes, k=5 thÆ°á»ng cho káº¿t quáº£ tá»‘t hÆ¡n k=3)")


# ==================== BÆ¯á»šC 4: HUáº¤N LUYá»†N VÃ€ KIá»‚M THá»¬ ====================
print("\n" + "=" * 80)
print("BÆ¯á»šC 4: HUáº¤N LUYá»†N VÃ€ KIá»‚M THá»¬")
print("=" * 80)

# Huáº¥n luyá»‡n
print("\nğŸ¯ Äang huáº¥n luyá»‡n mÃ´ hÃ¬nh...")
knn.fit(X_train_scaled, y_train)
print("âœ“ HoÃ n thÃ nh huáº¥n luyá»‡n!")

# Dá»± Ä‘oÃ¡n (cÃ³ thá»ƒ máº¥t vÃ i phÃºt vá»›i 20000 máº«u)
print(f"\nğŸ”® Äang dá»± Ä‘oÃ¡n trÃªn {len(X_test_scaled)} máº«u test...")
print("   (QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...)")
y_pred = knn.predict(X_test_scaled)
print("âœ“ HoÃ n thÃ nh dá»± Ä‘oÃ¡n!")

# ÄÃ¡nh giÃ¡
accuracy = accuracy_score(y_test, y_pred)
print(f"\nğŸ“Š Äá»˜ CHÃNH XÃC (ACCURACY): {accuracy * 100:.2f}%")

# Confusion Matrix (chá»‰ hiá»ƒn thá»‹ 10x10 Ä‘áº§u tiÃªn do quÃ¡ lá»›n)
cm = confusion_matrix(y_test, y_pred)
print(f"\nğŸ“ˆ KÃ­ch thÆ°á»›c Ma tráº­n nháº§m láº«n: {cm.shape}")

# Metrics tá»•ng quÃ¡t
precision_macro = precision_score(y_test, y_pred, average='macro')
recall_macro = recall_score(y_test, y_pred, average='macro')
f1_macro = f1_score(y_test, y_pred, average='macro')

print(f"\nğŸ“Š METRICS Tá»”NG QUÃT (Macro Average):")
print(f"  - Precision: {precision_macro*100:.2f}%")
print(f"  - Recall: {recall_macro*100:.2f}%")
print(f"  - F1-Score: {f1_macro*100:.2f}%")

# Chi tiáº¿t má»™t sá»‘ chá»¯ cÃ¡i
print(f"\nğŸ“‹ CHI TIáº¾T Má»˜T Sá» CHá»® CÃI:")
letters = label_encoder.classes_
for i in [0, 1, 2, 25]:  # A, B, C, Z
    indices = np.where(y_test == i)[0]
    if len(indices) > 0:
        y_test_letter = y_test[indices]
        y_pred_letter = y_pred[indices]
        acc = accuracy_score(y_test_letter, y_pred_letter)
        print(f"  Letter {letters[i]}: Accuracy = {acc*100:.2f}%")

# So sÃ¡nh má»™t sá»‘ dá»± Ä‘oÃ¡n
print(f"\nğŸ” SO SÃNH 10 Dá»° ÄOÃN Äáº¦U TIÃŠN:")
print(f"{'STT':<5} {'Thá»±c táº¿':<10} {'Dá»± Ä‘oÃ¡n':<10} {'Káº¿t quáº£':<10}")
print("-" * 40)
for i in range(min(10, len(y_test))):
    actual = letters[y_test[i]]
    predicted = letters[y_pred[i]]
    result = "âœ“ ÄÃºng" if y_test[i] == y_pred[i] else "âœ— Sai"
    print(f"{i+1:<5} {actual:<10} {predicted:<10} {result:<10}")


# ==================== BÆ¯á»šC 5: Tá»I Æ¯U HÃ“A ====================
print("\n" + "=" * 80)
print("BÆ¯á»šC 5: Tá»I Æ¯U HÃ“A - TÃŒM GIÃ TRá»Š K Tá»I Æ¯U")
print("=" * 80)

print("\nâš ï¸  LÆ¯U Ã: Vá»›i dataset lá»›n (20000 máº«u), viá»‡c thá»­ nhiá»u k tá»‘n thá»i gian.")
print("   ChÃºng ta sáº½ thá»­ k tá»« 1 Ä‘áº¿n 10 trÃªn táº­p test nhá» hÆ¡n.\n")

# Láº¥y subset nhá» Ä‘á»ƒ test nhanh
n_subset = min(1000, len(X_test_scaled))
X_test_subset = X_test_scaled[:n_subset]
y_test_subset = y_test[:n_subset]

k_values = range(1, 11)
accuracies = []

print(f"Äang thá»­ nghiá»‡m k tá»« 1 Ä‘áº¿n 10 trÃªn {n_subset} máº«u test...")
for k in k_values:
    knn_temp = KNearestNeighbors(k=k)
    knn_temp.fit(X_train_scaled, y_train)
    print(f"\nk={k}:")
    y_pred_temp = knn_temp.predict(X_test_subset)
    acc = accuracy_score(y_test_subset, y_pred_temp)
    accuracies.append(acc)
    print(f"  Accuracy = {acc*100:.2f}%")

best_k = k_values[np.argmax(accuracies)]
best_accuracy = max(accuracies)
print(f"\nğŸ† GIÃ TRá»Š K Tá»I Æ¯U: k={best_k} vá»›i accuracy={best_accuracy*100:.2f}%")


# ==================== BÆ¯á»šC 6: TRá»°C QUAN HÃ“A ====================
print("\n" + "=" * 80)
print("BÆ¯á»šC 6: TRá»°C QUAN HÃ“A Káº¾T QUáº¢")
print("=" * 80)

fig = plt.figure(figsize=(18, 10))

# 1. PhÃ¢n bá»‘ chá»¯ cÃ¡i trong dataset
ax1 = plt.subplot(2, 3, 1)
letter_counts = pd.Series(y).value_counts().sort_index()
plt.bar(range(26), letter_counts.values, color='steelblue', alpha=0.7)
plt.xlabel('Letter Index (A=0, Z=25)', fontsize=11)
plt.ylabel('Count', fontsize=11)
plt.title('PhÃ¢n bá»‘ 26 chá»¯ cÃ¡i trong dataset', fontsize=12, fontweight='bold')
plt.grid(True, alpha=0.3, axis='y')

# 2. Confusion Matrix (10x10 Ä‘áº§u tiÃªn)
ax2 = plt.subplot(2, 3, 2)
cm_subset = cm[:10, :10]
sns.heatmap(cm_subset, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})
plt.title('Confusion Matrix (A-J)', fontsize=12, fontweight='bold')
plt.ylabel('Actual', fontsize=11)
plt.xlabel('Predicted', fontsize=11)

# 3. Accuracy vs K
ax3 = plt.subplot(2, 3, 3)
plt.plot(k_values, [acc*100 for acc in accuracies], marker='o', linewidth=2, markersize=8, color='steelblue')
plt.axvline(x=best_k, color='red', linestyle='--', linewidth=2, label=f'Best k={best_k}')
plt.xlabel('GiÃ¡ trá»‹ K', fontsize=11)
plt.ylabel('Accuracy (%)', fontsize=11)
plt.title('Accuracy theo giÃ¡ trá»‹ K', fontsize=12, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.legend()

# 4. Feature Distribution - Width
ax4 = plt.subplot(2, 3, 4)
plt.hist(X[:, 2], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
plt.xlabel('Width', fontsize=11)
plt.ylabel('Frequency', fontsize=11)
plt.title('PhÃ¢n bá»‘ feature: Width', fontsize=12, fontweight='bold')
plt.grid(True, alpha=0.3, axis='y')

# 5. Feature Distribution - Height
ax5 = plt.subplot(2, 3, 5)
plt.hist(X[:, 3], bins=30, color='lightcoral', edgecolor='black', alpha=0.7)
plt.xlabel('Height', fontsize=11)
plt.ylabel('Frequency', fontsize=11)
plt.title('PhÃ¢n bá»‘ feature: Height', fontsize=12, fontweight='bold')
plt.grid(True, alpha=0.3, axis='y')

# 6. Accuracy by Letter (10 chá»¯ Ä‘áº§u)
ax6 = plt.subplot(2, 3, 6)
letter_accs = []
for i in range(10):
    indices = np.where(y_test == i)[0]
    if len(indices) > 0:
        acc = accuracy_score(y_test[indices], y_pred[indices])
        letter_accs.append(acc * 100)
    else:
        letter_accs.append(0)

plt.bar(range(10), letter_accs, color='seagreen', alpha=0.7)
plt.xlabel('Letter (A-J)', fontsize=11)
plt.ylabel('Accuracy (%)', fontsize=11)
plt.title('Accuracy theo tá»«ng chá»¯ cÃ¡i (A-J)', fontsize=12, fontweight='bold')
plt.xticks(range(10), letters[:10])
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('letter_knn_results.png', dpi=300, bbox_inches='tight')
print("\nâœ“ ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ vÃ o file: letter_knn_results.png")
plt.show()

print("\n" + "=" * 80)
print("âœ… HOÃ€N THÃ€NH Táº¤T Cáº¢ CÃC BÆ¯á»šC!")
print("=" * 80)
print("\nğŸ“Œ TÃ“M Táº®T:")
print(f"  - Dataset: 20,000 máº«u, 26 classes (A-Z), 16 features")
print(f"  - Accuracy tá»•ng quÃ¡t: {accuracy*100:.2f}%")
print(f"  - GiÃ¡ trá»‹ k tá»‘t nháº¥t: {best_k}")
print(f"  - Precision (macro): {precision_macro*100:.2f}%")
print(f"  - Recall (macro): {recall_macro*100:.2f}%")
print(f"  - F1-Score (macro): {f1_macro*100:.2f}%")